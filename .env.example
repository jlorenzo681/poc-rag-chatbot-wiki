# LLM Provider Configuration

# Groq API (Cloud-based, fast inference)
# Get your API key from: https://console.groq.com/keys
GROQ_API_KEY=your-groq-api-key-here

# Ollama Configuration (Local, containerized)
# Default URL when using docker-compose/podman-compose
OLLAMA_BASE_URL=http://ollama:11434
# For standalone mode (Ollama running on host): http://host.containers.internal:11434

# LM Studio Configuration (Local, GPU accelerated)
# Default URL when running LM Studio on your Mac
LMSTUDIO_BASE_URL=http://host.docker.internal:1234/v1
# Make sure to start LM Studio's local server (Local Server tab)

# Optional: OpenAI Embeddings (if not using HuggingFace)
# OPENAI_API_KEY=your-openai-api-key-here

# Optional: Default model settings
# DEFAULT_MODEL=llama-3.1-8b-instant
# DEFAULT_TEMPERATURE=0.3
# DEFAULT_EMBEDDING_MODEL=text-embedding-3-small

# Optional: Vector store settings
# VECTOR_STORE_PATH=faiss_index
# CHUNK_SIZE=1000
# CHUNK_OVERLAP=200
# RETRIEVAL_K=4
